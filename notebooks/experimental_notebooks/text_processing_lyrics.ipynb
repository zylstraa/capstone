{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = pd.read_csv('../genius_lyrics.csv') #the below only partially works, for bigram work see bigram notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using gensim simple_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "texts = [[text for text in doc.split()] for doc in l['lyrics']] #get a list of individual words, creates a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in texts for item in sublist] #convert our lists of lists to a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "ll = [simple_preprocess(text, deacc=True) for text in flat_list]\n",
    "# simple pre process cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testerl = l['lyrics'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import spacy # Generate list of tokens\n",
    "\n",
    "string = testerl\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "# Remove tokens that are not alphabetic\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() or lemma == '-PRON-']\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of stop words\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords]\n",
    "corpus = pd.Series(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-16f6eda65267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-744bb4b8217d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP tutorial from stackoverflow\n",
    "https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is the moment we needed the most?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(r'../lyrics.txt', encoding = 'utf-8')\n",
    "print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = l['lyrics'][0] #testing out the beginning of the tutorial\n",
    "# test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0363683d427d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_lyrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# test_lyrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "test_lyrics = nlp(test_dataset)\n",
    "# test_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at individual sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Where is the moment we needed the most?\n",
      "\n",
      "Sentence 2:\n",
      "\n",
      "\n",
      "\n",
      "Sentence 3:\n",
      "You kick up the leaves and the magic is lost\n",
      "\n",
      "\n",
      "Sentence 4:\n",
      "They tell me your blue skies fade to gray\n",
      "\n",
      "\n",
      "Sentence 5:\n",
      "They tell me your passion's gone away\n",
      "\n",
      "\n",
      "Sentence 6:\n",
      "And I don't need no carryin' on\n",
      "\n",
      "You stand in the line just to hit a new low\n",
      "\n",
      "\n",
      "Sentence 7:\n",
      "You're faking a smile with the coffee to go\n",
      "\n",
      "\n",
      "Sentence 8:\n",
      "You tell me your life's been way off line\n",
      "\n",
      "\n",
      "Sentence 9:\n",
      "You're falling to pieces every time\n",
      "\n",
      "\n",
      "Sentence 10:\n",
      "And I don't need no carryin' on\n",
      "\n",
      "Because you had a bad day\n",
      "\n",
      "\n",
      "Sentence 11:\n",
      "You're taking one down\n",
      "\n",
      "\n",
      "Sentence 12:\n",
      "You sing a sad song just to turn it around\n",
      "\n",
      "\n",
      "Sentence 13:\n",
      "You say you don't know\n",
      "\n",
      "\n",
      "Sentence 14:\n",
      "You tell me, don't lie\n",
      "\n",
      "\n",
      "Sentence 15:\n",
      "You work at a smile and you go for a ride\n",
      "\n",
      "\n",
      "Sentence 16:\n",
      "You had a bad day\n",
      "\n",
      "\n",
      "Sentence 17:\n",
      "The camera don't lie\n",
      "\n",
      "\n",
      "Sentence 18:\n",
      "You're coming back down\n",
      "\n",
      "Sentence 19:\n",
      "and you really don't mind\n",
      "\n",
      "\n",
      "Sentence 20:\n",
      "You had a bad day\n",
      "\n",
      "\n",
      "Sentence 21:\n",
      "You had a bad day\n",
      "\n",
      "\n",
      "Sentence 22:\n",
      "Will you need a blue sky holiday?\n",
      "\n",
      "Sentence 23:\n",
      "\n",
      "\n",
      "\n",
      "Sentence 24:\n",
      "The point is they laugh at what you say\n",
      "\n",
      "\n",
      "Sentence 25:\n",
      "And I don't need no carryin' on\n",
      "\n",
      "You had a bad day\n",
      "\n",
      "\n",
      "Sentence 26:\n",
      "You're taking one down\n",
      "\n",
      "\n",
      "Sentence 27:\n",
      "You sing a sad song just to turn it around\n",
      "\n",
      "\n",
      "Sentence 28:\n",
      "You say you don't know\n",
      "\n",
      "\n",
      "Sentence 29:\n",
      "You tell me, don't lie\n",
      "\n",
      "\n",
      "Sentence 30:\n",
      "You work at a smile and you go for a ride\n",
      "\n",
      "\n",
      "Sentence 31:\n",
      "You had a bad day\n",
      "\n",
      "\n",
      "Sentence 32:\n",
      "The camera don't lie\n",
      "\n",
      "\n",
      "Sentence 33:\n",
      "You're coming back down\n",
      "\n",
      "Sentence 34:\n",
      "and you really don't mind\n",
      "\n",
      "\n",
      "Sentence 35:\n",
      "You had a bad day\n",
      "\n",
      "\n",
      "Sentence 36:\n",
      "(Ooh, a holiday)\n",
      "\n",
      "\n",
      "\n",
      "Sentence 37:\n",
      "Sometimes the system goes on the blink\n",
      "\n",
      "\n",
      "Sentence 38:\n",
      "And the whole thing turns out wrong\n",
      "\n",
      "\n",
      "Sentence 39:\n",
      "You might not make it back and you know\n",
      "\n",
      "\n",
      "Sentence 40:\n",
      "That you could be well, oh, that strong\n",
      "\n",
      "\n",
      "Sentence 41:\n",
      "And I'm not wrong\n",
      "\n",
      "\n",
      "Sentence 42:\n",
      "(Yeah, yeah, yeah)\n",
      "\n",
      "\n",
      "Sentence 43:\n",
      "So where is the passion when you need it the most?\n",
      "\n",
      "Sentence 44:\n",
      "\n",
      "\n",
      "\n",
      "Sentence 45:\n",
      "Oh, you and I\n",
      "\n",
      "\n",
      "Sentence 46:\n",
      "You kick up the leaves and the magic is lost\n",
      "\n",
      "'Cause you had a bad day\n",
      "\n",
      "\n",
      "Sentence 47:\n",
      "You're taking one down\n",
      "\n",
      "\n",
      "Sentence 48:\n",
      "You sing a sad song just to turn it around\n",
      "\n",
      "\n",
      "Sentence 49:\n",
      "You say you don't know\n",
      "\n",
      "\n",
      "Sentence 50:\n",
      "You tell me don't lie\n",
      "\n",
      "\n",
      "Sentence 51:\n",
      "You work at a smile and you go for a ride\n",
      "\n",
      "\n",
      "Sentence 52:\n",
      "You had a bad day\n",
      "\n",
      "\n",
      "Sentence 53:\n",
      "You've seen what you're like\n",
      "\n",
      "\n",
      "Sentence 54:\n",
      "And how does it feel?\n",
      "\n",
      "Sentence 55:\n",
      "\n",
      "\n",
      "\n",
      "Sentence 56:\n",
      "One more time\n",
      "\n",
      "\n",
      "Sentence 57:\n",
      "You had a bad day\n",
      "\n",
      "\n",
      "Sentence 58:\n",
      "You had a bad day\n",
      "\n",
      "\n",
      "\n",
      "Sentence 59:\n",
      "Ah, yeah, yeah, yeah\n",
      "\n",
      "\n",
      "Sentence 60:\n",
      "Had a bad day (Ah)\n",
      "\n",
      "\n",
      "Sentence 61:\n",
      "Had a bad day (ah, yeah,\n",
      "\n",
      "Sentence 62:\n",
      "yeah, yeah)\n",
      "\n",
      "Sentence 63:\n",
      "\n",
      "\n",
      "\n",
      "Sentence 64:\n",
      "Had a bad day (Ah)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(test_lyrics.sents):\n",
    "    print( 'Sentence {}:'.format(num + 1))\n",
    "    print (sentence)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "named entities (not expecting any but it can occasionally appear in songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: a bad day - DATE\n",
      "\n",
      "Entity 2: a bad day - DATE\n",
      "\n",
      "Entity 3: a bad day - DATE\n",
      "\n",
      "Entity 4: a bad day - DATE\n",
      "\n",
      "Entity 5: a bad day - DATE\n",
      "\n",
      "Entity 6: a bad day - DATE\n",
      "\n",
      "Entity 7: a bad day - DATE\n",
      "\n",
      "Entity 8: a bad day - DATE\n",
      "\n",
      "Entity 9: a bad day - DATE\n",
      "\n",
      "Entity 10: One - CARDINAL\n",
      "\n",
      "Entity 11: a bad day - DATE\n",
      "\n",
      "Entity 12: a bad day - DATE\n",
      "\n",
      "Entity 13: a bad day - DATE\n",
      "\n",
      "Entity 14: a bad day - DATE\n",
      "\n",
      "Entity 15: a bad day - DATE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(test_lyrics.ents):\n",
    "    print ('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about part of speech tagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moment</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>bad</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>day</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Ah</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text part_of_speech\n",
       "0        Where            ADV\n",
       "1           is            AUX\n",
       "2          the            DET\n",
       "3       moment           NOUN\n",
       "4           we           PRON\n",
       "..         ...            ...\n",
       "505        bad            ADJ\n",
       "506        day           NOUN\n",
       "507          (          PUNCT\n",
       "508         Ah           INTJ\n",
       "509          )          PUNCT\n",
       "\n",
       "[510 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in test_lyrics]\n",
    "token_pos = [token.pos_ for token in test_lyrics]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_pos),\n",
    "             columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about text normalization, like stemming/lemmatization and shape analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_lyrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ca97c3dcb31f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken_lemma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_lyrics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtoken_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_lyrics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m token_lemma_df = pd.DataFrame(zip(token_text, token_lemma, token_shape),\n\u001b[0;32m      5\u001b[0m              columns=['token_text', 'token_lemma', 'token_shape'])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_lyrics' is not defined"
     ]
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in test_lyrics]\n",
    "token_shape = [token.shape_ for token in test_lyrics]\n",
    "\n",
    "token_lemma_df = pd.DataFrame(zip(token_text, token_lemma, token_shape),\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where',\n",
       " 'be',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'we',\n",
       " 'need',\n",
       " 'the',\n",
       " 'most',\n",
       " '?',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'kick',\n",
       " 'up',\n",
       " 'the',\n",
       " 'leave',\n",
       " 'and',\n",
       " 'the',\n",
       " 'magic',\n",
       " 'be',\n",
       " 'lose',\n",
       " '\\n',\n",
       " 'they',\n",
       " 'tell',\n",
       " 'I',\n",
       " 'your',\n",
       " 'blue',\n",
       " 'sky',\n",
       " 'fade',\n",
       " 'to',\n",
       " 'gray',\n",
       " '\\n',\n",
       " 'they',\n",
       " 'tell',\n",
       " 'I',\n",
       " 'your',\n",
       " 'passion',\n",
       " 'be',\n",
       " 'go',\n",
       " 'away',\n",
       " '\\n',\n",
       " 'and',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'need',\n",
       " 'no',\n",
       " 'carryin',\n",
       " \"'\",\n",
       " 'on',\n",
       " '\\n\\n',\n",
       " 'you',\n",
       " 'stand',\n",
       " 'in',\n",
       " 'the',\n",
       " 'line',\n",
       " 'just',\n",
       " 'to',\n",
       " 'hit',\n",
       " 'a',\n",
       " 'new',\n",
       " 'low',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'be',\n",
       " 'fake',\n",
       " 'a',\n",
       " 'smile',\n",
       " 'with',\n",
       " 'the',\n",
       " 'coffee',\n",
       " 'to',\n",
       " 'go',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'I',\n",
       " 'your',\n",
       " 'life',\n",
       " 'be',\n",
       " 'be',\n",
       " 'way',\n",
       " 'off',\n",
       " 'line',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'be',\n",
       " 'fall',\n",
       " 'to',\n",
       " 'piece',\n",
       " 'every',\n",
       " 'time',\n",
       " '\\n',\n",
       " 'and',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'need',\n",
       " 'no',\n",
       " 'carryin',\n",
       " \"'\",\n",
       " 'on',\n",
       " '\\n\\n',\n",
       " 'because',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'be',\n",
       " 'take',\n",
       " 'one',\n",
       " 'down',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'sing',\n",
       " 'a',\n",
       " 'sad',\n",
       " 'song',\n",
       " 'just',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'it',\n",
       " 'around',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'say',\n",
       " 'you',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'I',\n",
       " ',',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'lie',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'work',\n",
       " 'at',\n",
       " 'a',\n",
       " 'smile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'go',\n",
       " 'for',\n",
       " 'a',\n",
       " 'ride',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " 'the',\n",
       " 'camera',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'lie',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'be',\n",
       " 'come',\n",
       " 'back',\n",
       " 'down',\n",
       " 'and',\n",
       " 'you',\n",
       " 'really',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mind',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " 'will',\n",
       " 'you',\n",
       " 'need',\n",
       " 'a',\n",
       " 'blue',\n",
       " 'sky',\n",
       " 'holiday',\n",
       " '?',\n",
       " '\\n',\n",
       " 'the',\n",
       " 'point',\n",
       " 'be',\n",
       " 'they',\n",
       " 'laugh',\n",
       " 'at',\n",
       " 'what',\n",
       " 'you',\n",
       " 'say',\n",
       " '\\n',\n",
       " 'and',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'need',\n",
       " 'no',\n",
       " 'carryin',\n",
       " \"'\",\n",
       " 'on',\n",
       " '\\n\\n',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'be',\n",
       " 'take',\n",
       " 'one',\n",
       " 'down',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'sing',\n",
       " 'a',\n",
       " 'sad',\n",
       " 'song',\n",
       " 'just',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'it',\n",
       " 'around',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'say',\n",
       " 'you',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'I',\n",
       " ',',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'lie',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'work',\n",
       " 'at',\n",
       " 'a',\n",
       " 'smile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'go',\n",
       " 'for',\n",
       " 'a',\n",
       " 'ride',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " 'the',\n",
       " 'camera',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'lie',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'be',\n",
       " 'come',\n",
       " 'back',\n",
       " 'down',\n",
       " 'and',\n",
       " 'you',\n",
       " 'really',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mind',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " '(',\n",
       " 'Ooh',\n",
       " ',',\n",
       " 'a',\n",
       " 'holiday',\n",
       " ')',\n",
       " '\\n\\n',\n",
       " 'sometimes',\n",
       " 'the',\n",
       " 'system',\n",
       " 'go',\n",
       " 'on',\n",
       " 'the',\n",
       " 'blink',\n",
       " '\\n',\n",
       " 'and',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'thing',\n",
       " 'turn',\n",
       " 'out',\n",
       " 'wrong',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'might',\n",
       " 'not',\n",
       " 'make',\n",
       " 'it',\n",
       " 'back',\n",
       " 'and',\n",
       " 'you',\n",
       " 'know',\n",
       " '\\n',\n",
       " 'that',\n",
       " 'you',\n",
       " 'could',\n",
       " 'be',\n",
       " 'well',\n",
       " ',',\n",
       " 'oh',\n",
       " ',',\n",
       " 'that',\n",
       " 'strong',\n",
       " '\\n',\n",
       " 'and',\n",
       " 'I',\n",
       " 'be',\n",
       " 'not',\n",
       " 'wrong',\n",
       " '\\n',\n",
       " '(',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'yeah',\n",
       " ')',\n",
       " '\\n',\n",
       " 'so',\n",
       " 'where',\n",
       " 'be',\n",
       " 'the',\n",
       " 'passion',\n",
       " 'when',\n",
       " 'you',\n",
       " 'need',\n",
       " 'it',\n",
       " 'the',\n",
       " 'most',\n",
       " '?',\n",
       " '\\n',\n",
       " 'oh',\n",
       " ',',\n",
       " 'you',\n",
       " 'and',\n",
       " 'I',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'kick',\n",
       " 'up',\n",
       " 'the',\n",
       " 'leave',\n",
       " 'and',\n",
       " 'the',\n",
       " 'magic',\n",
       " 'be',\n",
       " 'lose',\n",
       " '\\n\\n',\n",
       " \"'cause\",\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'be',\n",
       " 'take',\n",
       " 'one',\n",
       " 'down',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'sing',\n",
       " 'a',\n",
       " 'sad',\n",
       " 'song',\n",
       " 'just',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'it',\n",
       " 'around',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'say',\n",
       " 'you',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'lie',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'work',\n",
       " 'at',\n",
       " 'a',\n",
       " 'smile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'go',\n",
       " 'for',\n",
       " 'a',\n",
       " 'ride',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'see',\n",
       " 'what',\n",
       " 'you',\n",
       " 'be',\n",
       " 'like',\n",
       " '\\n',\n",
       " 'and',\n",
       " 'how',\n",
       " 'do',\n",
       " 'it',\n",
       " 'feel',\n",
       " '?',\n",
       " '\\n',\n",
       " 'one',\n",
       " 'more',\n",
       " 'time',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '\\n\\n',\n",
       " 'ah',\n",
       " ',',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'yeah',\n",
       " '\\n',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '(',\n",
       " 'ah',\n",
       " ')',\n",
       " '\\n',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '(',\n",
       " 'ah',\n",
       " ',',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'yeah',\n",
       " ')',\n",
       " '\\n',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " '(',\n",
       " 'ah',\n",
       " ')']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about token-level entity analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moment</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>bad</td>\n",
       "      <td>DATE</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>day</td>\n",
       "      <td>DATE</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>(</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Ah</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>)</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text entity_type inside_outside_begin\n",
       "0        Where                                O\n",
       "1           is                                O\n",
       "2          the                                O\n",
       "3       moment                                O\n",
       "4           we                                O\n",
       "..         ...         ...                  ...\n",
       "505        bad        DATE                    I\n",
       "506        day        DATE                    I\n",
       "507          (                                O\n",
       "508         Ah                                O\n",
       "509          )                                O\n",
       "\n",
       "[510 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in test_lyrics]\n",
    "token_entity_iob = [token.ent_iob_ for token in test_lyrics]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_entity_type, token_entity_iob),\n",
    "             columns = ['token_text','entity_type','inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What about a variety of other token-level attributes, such as the relative frequency of tokens, and whether or not a token matches any of these categories?\n",
    "\n",
    "* stopword\n",
    "* punctuation\n",
    "* whitespace\n",
    "* represents a number\n",
    "* whether or not the token is included in spaCy's default vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moment</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>bad</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>day</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>(</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Ah</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>)</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "0     Where            -20.0   Yes                                    \n",
       "1        is            -20.0   Yes                                    \n",
       "2       the            -20.0   Yes                                    \n",
       "3    moment            -20.0                                          \n",
       "4        we            -20.0   Yes                                    \n",
       "..      ...              ...   ...          ...         ...     ...   \n",
       "505     bad            -20.0                                          \n",
       "506     day            -20.0                                          \n",
       "507       (            -20.0                Yes                       \n",
       "508      Ah            -20.0                                          \n",
       "509       )            -20.0                Yes                       \n",
       "\n",
       "    out of vocab?  \n",
       "0             Yes  \n",
       "1             Yes  \n",
       "2             Yes  \n",
       "3             Yes  \n",
       "4             Yes  \n",
       "..            ...  \n",
       "505           Yes  \n",
       "506           Yes  \n",
       "507           Yes  \n",
       "508           Yes  \n",
       "509           Yes  \n",
       "\n",
       "[510 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in test_lyrics]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                 columns = [\n",
    "                     'text',\n",
    "                     'log_probability',\n",
    "                     'stop?',\n",
    "                     'punctuation?',\n",
    "                     'whitespace?',\n",
    "                     'number?',\n",
    "                     'out of vocab?']\n",
    "                 )\n",
    "\n",
    "df.loc[:,'stop?':'out of vocab?'] = (df.loc[:,'stop?':'out of vocab?'].applymap(lambda x: u'Yes' if x else u''))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#get the filepath\n",
    "\n",
    "with codecs.open('../lyrics.txt') as f:\n",
    "    first_song_lyrics = f.readline()\n",
    "    \n",
    "print(first_song_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "# def line_lyrics(filename):\n",
    "#     \"\"\"\n",
    "#     generator function to read in lyrics from the file\n",
    "#     and un-escape the original line breaks in the text\n",
    "#     \"\"\"\n",
    "    \n",
    "#     with codecs.open(filename, encoding='utf_8') as f:\n",
    "#         for lyrics in f:\n",
    "#             yield lyrics # don't need this because my txt file is already broken into lines\n",
    "            \n",
    "# def lemmatized_sentence_corpus(filename):\n",
    "#     \"\"\"\n",
    "#     generator function to use spaCy to parse lyrics,\n",
    "#     lemmatize the text, and yield sentences\n",
    "#     \"\"\"\n",
    "    \n",
    "#     for parsed_lyrics in nlp.pipe(line_lyrics(filename),\n",
    "#                                   batch_size=10000):\n",
    "        \n",
    "#         for sent in parsed_lyrics.sents:\n",
    "#             yield u' '.join([token.lemma_ for token in sent\n",
    "#                              if not punct_space(token)]) #shouldn't need this either as it's broken into sentences already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence('../lyrics.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u''.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save('../bigrams.txt')\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load('../bigrams.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open('../bigrams.txt', 'w') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence('../bigrams.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2070\u001b[0m             \u001b[1;31m# Things that don't have seek will trigger an exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2071\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2072\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-416d21e03d8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbigram_sentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m230\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m240\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2080\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2081\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2082\u001b[1;33m                     \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2083\u001b[0m                     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2084\u001b[0m                     \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-970e3142ef22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'generator' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpus = []\n",
    "path = '.'\n",
    "\n",
    "for i in os.walk(path).next()[2]:\n",
    "    if i.endswith('.txt'):\n",
    "        f = open(os.path.join(path,i))\n",
    "        corpus.append(f.read())\n",
    "        \n",
    "frequencies = Counter([])\n",
    "\n",
    "for text in corpus:\n",
    "    token = nltk.word_tokenize(text)\n",
    "    bigrams = ngrams(token, 2)\n",
    "    frequencies += Counter(bigrams)\n",
    "    \n",
    "# https://stackoverflow.com/questions/32441605/generating-ngrams-unigrams-bigrams-etc-from-a-large-corpus-of-txt-files-and-t/32442106    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv('../genius_lyrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = lyrics['lyrics'].tolist() #list of Unicode strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "#Removing stopwords:\n",
    "from sklearn.feature_extraction import text\n",
    "sw = ['oh','ooh','yeah','na','la','hey','like','da', 'feat',\n",
    "      'whoa','uh','huh','doh','doo','ha','eh','ay','ayy','ll','re','ve','ft', 'yo',\n",
    "     'mmm','nah','ah','woah'] #removing sounds, contractions and featured artists\n",
    "stop_words= text.ENGLISH_STOP_WORDS.union(sw)\n",
    "\n",
    "docs = [[token for token in doc if token not in stop_words] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low_low</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dr_manhattan</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>don_care</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shake_shake</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tell_em</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>work_work</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>night_long</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bad_bitch</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>don_understand</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>don_worry</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kendrick_lamar</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dat_soulja</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>turn_light</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>little_bit</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>make_rain</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>walk_away</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bad_bad</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>walk_walk</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>thinking_bout</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dance_dance</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>run_away</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>boom_boom</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>live_life</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>machine_gun</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>denzel_curry</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kanye_west</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>young_dumb</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tonight_tonight</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>somethin_bout</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>post_malone</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             phrase  count\n",
       "0           low_low    193\n",
       "1      dr_manhattan    176\n",
       "2          don_care    162\n",
       "3       shake_shake    128\n",
       "4           tell_em    119\n",
       "5         work_work    111\n",
       "6        night_long     93\n",
       "7         bad_bitch     89\n",
       "8    don_understand     83\n",
       "9         don_worry     81\n",
       "10   kendrick_lamar     80\n",
       "11       dat_soulja     80\n",
       "12       turn_light     79\n",
       "13       little_bit     79\n",
       "14        make_rain     77\n",
       "15        walk_away     77\n",
       "16          bad_bad     75\n",
       "17        walk_walk     75\n",
       "18    thinking_bout     74\n",
       "19      dance_dance     73\n",
       "20         run_away     72\n",
       "21        boom_boom     71\n",
       "22        live_life     70\n",
       "23      machine_gun     70\n",
       "24     denzel_curry     69\n",
       "25       kanye_west     69\n",
       "26       young_dumb     68\n",
       "27  tonight_tonight     68\n",
       "28    somethin_bout     68\n",
       "29      post_malone     68"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgs = pd.Series([x for x in [item for sublist in docs for item in sublist] if '_' in x])\n",
    "bgs = pd.DataFrame(bgs.value_counts()).reset_index().rename(columns={'index':'phrase',0:'count'})\n",
    "bgs.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# #code below is not helpful\n",
    "\n",
    "# p = []\n",
    "# s = []\n",
    "\n",
    "# for phrase, score in phrases.find_phrases(docs).items():\n",
    "#     p.append(phrase)\n",
    "#     s.append(score)\n",
    "\n",
    "# bigrams = pd.DataFrame()\n",
    "# bigrams['phrase'] = pd.Series(p)\n",
    "# bigrams['score'] = pd.Series(s)\n",
    "\n",
    "# bigrams.sort_values('score',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
